{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BinBin264/fish_classification/blob/BE/Fish_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Fish Image Species Classification  \n",
        "\n"
      ],
      "metadata": {
        "id": "5ngp2NCP5Y8T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Jrf2uHVvVP_",
        "outputId": "2003e37b-792b-43b6-cb34-f4aa6d1d4104"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import os.path\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "3vqzU5_q00tU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  load species dataset"
      ],
      "metadata": {
        "id": "k_s2JBTo5fCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dir = Path('../input/fish-species/Species/Test_Set')\n",
        "\n",
        "filepaths = list(dir.glob(r'**/*.jpg'))\n",
        "labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n",
        "\n",
        "\n",
        "filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
        "labels = pd.Series(labels, name='Label')\n",
        "dataframe2_test = pd.concat([filepaths , labels] , axis=1)\n",
        "dataframe2_test\n"
      ],
      "metadata": {
        "id": "LmlNTL0n00zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dir = Path('../input/fish-species/Species/Training_Set')\n",
        "\n",
        "filepaths = list(dir.glob(r'**/*.jpg'))\n",
        "labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n",
        "\n",
        "\n",
        "filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
        "labels = pd.Series(labels, name='Label')\n",
        "dataframe2_train = pd.concat([filepaths , labels] , axis=1)\n",
        "dataframe2_train"
      ],
      "metadata": {
        "id": "2p4Xg0P847bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe2_train['Label'].value_counts()"
      ],
      "metadata": {
        "id": "mSULV1ZI4-li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe2_test['Label'].value_counts()"
      ],
      "metadata": {
        "id": "0qoc8w_75BIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe2_test"
      ],
      "metadata": {
        "id": "uauSCRfB5G9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = pd.concat([dataframe2_train,dataframe2_test],axis =0 )"
      ],
      "metadata": {
        "id": "Vk_NVTlA5Hoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe['Label'].value_counts()"
      ],
      "metadata": {
        "id": "2Db1Mo3Y5JjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "samples = []\n",
        "for category in dataframe['Label'].unique():\n",
        "    category_slice = dataframe.query(\"Label == @category\")\n",
        "    samples.append(category_slice.sample(500, random_state=1))\n",
        "\n",
        "dataframe = pd.concat(samples, axis=0).sample(frac=1.0, random_state=1).reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "fKROxSC15OR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe['Label'].value_counts()"
      ],
      "metadata": {
        "id": "Fd20K8yz5Pcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating train, test DataFrames"
      ],
      "metadata": {
        "id": "NA6y64VL5ouR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = train_test_split(dataframe, train_size=0.9, shuffle=True, random_state=1)"
      ],
      "metadata": {
        "id": "7pbcE6kv5RTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape"
      ],
      "metadata": {
        "id": "BKxFm4hz5r8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the Images for mobilenetv2\n"
      ],
      "metadata": {
        "id": "Y3QARNjt7DuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n",
        "    height_shift_range=0.2,\n",
        "    width_shift_range=0.2,\n",
        "    rotation_range=40,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "\n",
        "test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
        ")"
      ],
      "metadata": {
        "id": "UFJebvIc7EOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = train_generator.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    x_col='Filepath',\n",
        "    y_col='Label',\n",
        "    target_size=(160, 160),\n",
        "    color_mode='rgb',\n",
        "    class_mode='categorical',\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        "    seed=42,\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_images = train_generator.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    x_col='Filepath',\n",
        "    y_col='Label',\n",
        "    target_size=(160, 160),\n",
        "    color_mode='rgb',\n",
        "    class_mode='categorical',\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        "    seed=42,\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "test_images = test_generator.flow_from_dataframe(\n",
        "    dataframe=test_df,\n",
        "    x_col='Filepath',\n",
        "    y_col='Label',\n",
        "    target_size=(160, 160),\n",
        "    color_mode='rgb',\n",
        "    class_mode='categorical',\n",
        "    batch_size=128,\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "svyEg9xX7Ghn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_images.class_indices"
      ],
      "metadata": {
        "id": "nBvlbr977ILs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Pretrained MobileNetV2 Model"
      ],
      "metadata": {
        "id": "-SPp5rBM7MCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "pretrained_model = tf.keras.applications.MobileNetV2(\n",
        "    input_shape=(224, 224, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    pooling='avg'\n",
        ")\n",
        "\n",
        "pretrained_model.trainable = False\n"
      ],
      "metadata": {
        "id": "-j-bL3rR7JsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "zlHEfnSy7Qzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential ([\n",
        "                        pretrained_model,\n",
        "                        Flatten(),\n",
        "                        Dropout(0.2),\n",
        "                        Dense(128, activation='relu'),\n",
        "                        Dense(64, activation='relu'),\n",
        "                        Dense(20, activation='softmax')\n",
        "                    ])\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "uNcb7ATr7O80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "i7avpFBJ7VB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate schedule\n",
        "initial_learning_rate = 0.001\n",
        "decay_steps = 1000\n",
        "decay_rate = 0.9\n",
        "\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate,\n",
        "    decay_steps=decay_steps,\n",
        "    decay_rate=decay_rate,\n",
        "    staircase=True\n",
        ")\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Model checkpoint\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    'best_model_mbn.keras',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    mode='max'\n",
        ")"
      ],
      "metadata": {
        "id": "E_KiebsY7XpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_images,\n",
        "                    validation_data=val_images,\n",
        "                    epochs=30,\n",
        "                    callbacks=[early_stopping, checkpoint])"
      ],
      "metadata": {
        "id": "EgXhearb7YEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('last_mobile.keras')"
      ],
      "metadata": {
        "id": "x7UTezl17drw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "lmn6w9CW7eMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_images, verbose=0)\n",
        "\n",
        "print(\"    Test Loss: {:.5f}\".format(results[0]))\n",
        "print(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['Training', 'Validation'])\n",
        "plt.title('Training and Validation losses')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.legend(['Training', 'Validation'])\n",
        "plt.title('Training and Validation binary_accuracy')\n",
        "plt.xlabel('epoch')\n"
      ],
      "metadata": {
        "id": "pppAGS8T7eDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(test_images)\n",
        "pred = np.argmax(pred, axis=1)"
      ],
      "metadata": {
        "id": "DdHzVhWC73eV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_data=test_df.copy()\n",
        "labels={}\n",
        "for l,v in test_images.class_indices.items():\n",
        "    labels.update({v:l})\n",
        "predict_data['pred']=pred\n",
        "predict_data['pred']=predict_data['pred'].apply(lambda x: labels[x])"
      ],
      "metadata": {
        "id": "2xUMwJph77IT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_data=predict_data.reset_index(drop=True)\n",
        "predict_data.head(10)\n",
        "predict_data=predict_data.reset_index(drop=True)\n",
        "predict_data.head(10)"
      ],
      "metadata": {
        "id": "n2TwwYLj7-Ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_data[predict_data['Label']!=predict_data['pred']]"
      ],
      "metadata": {
        "id": "ycUqz1YP7_4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "print(f\"Accuracy Score: {accuracy_score(predict_data['Label'],predict_data['pred'])}\")\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(confusion_matrix(predict_data['Label'],predict_data['pred']), annot=True, fmt='2d')"
      ],
      "metadata": {
        "id": "rh2gUVkB8BWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(predict_data['Label'],predict_data['pred']))"
      ],
      "metadata": {
        "id": "QJlfo5Ld8Nu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine tunning\n"
      ],
      "metadata": {
        "id": "5w6rd_D08UJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def mobilenet_like_model(input_shape=(224, 224, 3), num_classes=20):\n",
        "    \"\"\"\n",
        "    Create a MobileNetV2-like model for classification\n",
        "\n",
        "    Args:\n",
        "        input_shape (tuple): Shape of input images\n",
        "        num_classes (int): Number of output classes\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: Compiled neural network model\n",
        "    \"\"\"\n",
        "    def _depthwise_conv_block(inputs, filters, kernel=3, strides=1, expansion=6, block_id=None):\n",
        "        \"\"\"\n",
        "        Depthwise Separable Convolution Block\n",
        "\n",
        "        Args:\n",
        "            inputs (tensor): Input tensor\n",
        "            filters (int): Number of output filters\n",
        "            kernel (int): Kernel size for depthwise convolution\n",
        "            strides (int): Strides for convolution\n",
        "            expansion (int): Expansion factor for intermediate layer\n",
        "            block_id (int): Identifier for the block\n",
        "\n",
        "        Returns:\n",
        "            tensor: Output tensor after convolution block\n",
        "        \"\"\"\n",
        "        input_shape = tf.keras.backend.int_shape(inputs)\n",
        "\n",
        "        # Expansion layer\n",
        "        x = layers.Conv2D(\n",
        "            int(input_shape[-1] * expansion),\n",
        "            kernel_size=1,\n",
        "            padding='same',\n",
        "            use_bias=False,\n",
        "            activation=None,\n",
        "            name=f'block_{block_id}_expand'\n",
        "        )(inputs)\n",
        "        x = layers.BatchNormalization(name=f'block_{block_id}_expand_BN')(x)\n",
        "        x = layers.ReLU(6., name=f'block_{block_id}_expand_relu')(x)\n",
        "\n",
        "        # Depthwise convolution\n",
        "        x = layers.DepthwiseConv2D(\n",
        "            kernel_size=kernel,\n",
        "            strides=strides,\n",
        "            depth_multiplier=1,\n",
        "            padding='same',\n",
        "            use_bias=False,\n",
        "            name=f'block_{block_id}_depthwise'\n",
        "        )(x)\n",
        "        x = layers.BatchNormalization(name=f'block_{block_id}_depthwise_BN')(x)\n",
        "        x = layers.ReLU(6., name=f'block_{block_id}_depthwise_relu')(x)\n",
        "\n",
        "        # Projection layer\n",
        "        x = layers.Conv2D(\n",
        "            filters,\n",
        "            kernel_size=1,\n",
        "            padding='same',\n",
        "            use_bias=False,\n",
        "            activation=None,\n",
        "            name=f'block_{block_id}_project'\n",
        "        )(x)\n",
        "        x = layers.BatchNormalization(name=f'block_{block_id}_project_BN')(x)\n",
        "\n",
        "        # Residual connection if input and output shapes match\n",
        "        if tf.keras.backend.int_shape(inputs)[-1] == filters and strides == 1:\n",
        "            x = layers.Add(name=f'block_{block_id}_add')([inputs, x])\n",
        "\n",
        "        return x\n",
        "\n",
        "    # Input layer\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Initial convolution\n",
        "    x = layers.Conv2D(32, kernel_size=3, strides=2, padding='same', use_bias=False, name='Conv1')(inputs)\n",
        "    x = layers.BatchNormalization(name='bn_Conv1')(x)\n",
        "    x = layers.ReLU(6., name='Conv1_relu')(x)\n",
        "\n",
        "    # Depthwise separable convolution blocks\n",
        "    x = _depthwise_conv_block(x, 16, block_id=1, strides=1)\n",
        "    x = _depthwise_conv_block(x, 24, block_id=2, strides=2)\n",
        "    x = _depthwise_conv_block(x, 24, block_id=3, strides=1)\n",
        "    x = _depthwise_conv_block(x, 32, block_id=4, strides=2)\n",
        "    x = _depthwise_conv_block(x, 32, block_id=5, strides=1)\n",
        "    x = _depthwise_conv_block(x, 32, block_id=6, strides=1)\n",
        "    x = _depthwise_conv_block(x, 64, block_id=7, strides=2)\n",
        "    x = _depthwise_conv_block(x, 64, block_id=8, strides=1)\n",
        "    x = _depthwise_conv_block(x, 96, block_id=9, strides=1)\n",
        "    x = _depthwise_conv_block(x, 96, block_id=10, strides=1)\n",
        "    x = _depthwise_conv_block(x, 160, block_id=11, strides=2)\n",
        "    x = _depthwise_conv_block(x, 160, block_id=12, strides=1)\n",
        "    x = _depthwise_conv_block(x, 320, block_id=13, strides=1)\n",
        "\n",
        "    # Final convolution and pooling\n",
        "    x = layers.Conv2D(1280, kernel_size=1, padding='same', use_bias=False, name='Conv_1')(x)\n",
        "    x = layers.BatchNormalization(name='Conv_1_bn')(x)\n",
        "    x = layers.ReLU(6., name='out_relu')(x)\n",
        "\n",
        "    # Global average pooling\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "    # Fully connected layers\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dropout(0.3)(x)  # Increased dropout for regularization\n",
        "    x = layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "    x = layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Create model\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Compile model\n",
        "    initial_learning_rate = 0.001\n",
        "    decay_steps = 1000\n",
        "    decay_rate = 0.9\n",
        "\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate,\n",
        "        decay_steps=decay_steps,\n",
        "        decay_rate=decay_rate,\n",
        "        staircase=True\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define callbacks\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    'best_model_mbn_build.keras',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    mode='max'\n",
        ")\n",
        "\n",
        "# Create the model\n",
        "model = mobilenet_like_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "SuinSpd-8VxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate schedule\n",
        "initial_learning_rate = 0.001\n",
        "decay_steps = 1000\n",
        "decay_rate = 0.9\n",
        "\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate,\n",
        "    decay_steps=decay_steps,\n",
        "    decay_rate=decay_rate,\n",
        "    staircase=True\n",
        ")\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Model checkpoint\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    'best_model_mbn_build.keras',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    mode='max'\n",
        ")"
      ],
      "metadata": {
        "id": "5wnaTQdE8aZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "history = model2.fit(train_images,\n",
        "                    validation_data=val_images,\n",
        "                    epochs=30,\n",
        "                    callbacks=[early_stopping, checkpoint])"
      ],
      "metadata": {
        "id": "ovQCYNLB8a1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = model2.evaluate(test_images, verbose=0)\n",
        "\n",
        "print(\"    Test Loss: {:.5f}\".format(results[0]))\n",
        "print(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))"
      ],
      "metadata": {
        "id": "PDqwBt-N8dXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['Training', 'Validation'])\n",
        "plt.title('Training and Validation losses')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.legend(['Training', 'Validation'])\n",
        "plt.title('Training and Validation binary_accuracy')\n",
        "plt.xlabel('epoch')\n"
      ],
      "metadata": {
        "id": "MMPOE0uJ8frQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model2.predict(test_images)\n",
        "pred = np.argmax(pred, axis=1)"
      ],
      "metadata": {
        "id": "-WdF0zJJ8i4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_data=test_df.copy()\n",
        "labels={}\n",
        "for l,v in test_images.class_indices.items():\n",
        "    labels.update({v:l})\n",
        "predict_data['pred']=pred\n",
        "predict_data['pred']=predict_data['pred'].apply(lambda x: labels[x])"
      ],
      "metadata": {
        "id": "gJY0g1xT8kek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_data=predict_data.reset_index(drop=True)\n",
        "predict_data.head(10)\n",
        "predict_data=predict_data.reset_index(drop=True)\n",
        "predict_data.head(10)"
      ],
      "metadata": {
        "id": "fuN-rJzY8nfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_data[predict_data['Label']!=predict_data['pred']]"
      ],
      "metadata": {
        "id": "ez48HHHy8oxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "print(f\"Accuracy Score: {accuracy_score(predict_data['Label'],predict_data['pred'])}\")\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(confusion_matrix(predict_data['Label'],predict_data['pred']), annot=True, fmt='2d')"
      ],
      "metadata": {
        "id": "eS_JtmkT8s_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(predict_data['Label'],predict_data['pred']))"
      ],
      "metadata": {
        "id": "XJzruZ1u8tUl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}